
embedding:
  mode: ${EMBEDDING_MODE:huggingface}
  huggingface_model: ${EMBEDDING_HUGGINGFACE_MODEL:BAAI/bge-small-zh}
  ingest_mode: simple
  embed_dim: 768

llm:
  mode: ${LLM_MODE:ollama}
  ollama_model: ${LLM_OLLAMA_MODEL:llama3.2:3b}

ollama:
  llm_model: ${OLLAMA_LLM_MODEL:llama3.2:3b}
  embedding_model: ${OLLAMA_EMBEDDING_MODEL:nomic-embed-text}
  #容器
  api_base: ${OLLAMA_API_BASE:http://ollama-server:11434}  # 显式指定容器名更稳妥
  embedding_api_base: ${OLLAMA_EMBEDDING_API_BASE:http://ollama-server:11434}
  #本地
  #api_base: ${OLLAMA_API_BASE:http://localhost:11434/}
  #embedding_api_base: ${OLLAMA_EMBEDDING_API_BASE:http://localhost:11434/}
  tfs_z: ${OLLAMA_TFS_Z:1.0}
  top_k: ${OLLAMA_TOP_K:40}
  top_p: ${OLLAMA_TOP_P:0.9}
  num_predict: ${OLLAMA_NUM_PREDICT:512}
  repeat_last_n: ${OLLAMA_REPEAT_LAST_N:64}
  repeat_penalty: ${OLLAMA_REPEAT_PENALTY:1.2}
  request_timeout: ${OLLAMA_REQUEST_TIMEOUT:600.0}
  autopull_models: ${OLLAMA_AUTOPULL_MODELS:true}
  temperature: ${OLLAMA_TEMPERATURE:0.0}
  context_window: ${OLLAMA_CONTEXT_WINDOW:2048}
  keep_alive: ${OLLAMA_KEEP_ALIVE:5m}

vectorstore:
  database: qdrant


qdrant:
  #path: local_data/ollama3/qdrant
  #docker 配置
  path: /app/data

nodestore:
  database: simple

data:
  local_data_folder: local_data/ollama3

rag:
  similarity_top_k: 2
  similarity_value: 0.45
  rerank:
    enabled: false
    model: cross-encoder/ms-marco-MiniLM-L-2-v2
    top_n: 1