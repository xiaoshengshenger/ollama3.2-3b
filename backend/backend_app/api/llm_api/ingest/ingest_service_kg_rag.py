import os
import re
import tempfile
import logging
import time
from pathlib import Path
from typing import TYPE_CHECKING, AnyStr, BinaryIO, List, Optional, Tuple
from dataclasses import dataclass
from backend_app.constants import get_local_kg_data_path 

# é¡¹ç›®å†…éƒ¨ä¾èµ–
from injector import inject, singleton
from backend_app.api.LLM.llm_component import LLMComponent
from backend_app.api.Embedding.embedding_component import EmbeddingComponent
from backend_app.api.LLM.node_store_component import NodeKgStoreComponent
from backend_app.api.llm_api.ingest.model import IngestedDoc
from backend_app.api.settings.settings import settings

# LlamaIndex æ ¸å¿ƒä¾èµ–
from llama_index.core import load_index_from_storage, StorageContext
from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import Document as LlamaDoc
from llama_index.core.storage.docstore.types import RefDocInfo
from llama_index.graph_stores.neo4j import Neo4jGraphStore

from backend_app.api.LLM.vector_store_component import (
    VectorStoreComponent,
)

if TYPE_CHECKING:
    from llama_index.core import QueryEngine
    from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex
import datetime

logger = logging.getLogger(__name__)

# ====================== é…ç½®ç±»ï¼ˆè§£è€¦Neo4jè¿æ¥ï¼‰ ======================
@dataclass
class Neo4jConfig:
    """Neo4j è¿æ¥é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡/é¡¹ç›®é…ç½®è¯»å–ï¼‰"""
    username: str = os.getenv("NEO4J_USER", settings().neo4j.username)
    password: str = os.getenv("NEO4J_PASSWORD", settings().neo4j.password)
    url: str = os.getenv("NEO4J_URL", settings().neo4j.url)
    database: str = os.getenv("NEO4J_DB", settings().neo4j.database)
    max_triplets_per_chunk: int = int(os.getenv("NEO4J_MAX_TRIPLETS", 3))
    include_embeddings: bool = os.getenv("NEO4J_INCLUDE_EMBEDDINGS", "True") == "True"

# ====================== å›ºå®šç´¢å¼•å¸¸é‡ ======================
KG_RAG_INDEX_ID = "kg_rag_index"  # å®šä¹‰å›ºå®šç´¢å¼•ID

# ====================== çŸ¥è¯†å›¾è°±RAGæœåŠ¡ï¼ˆå•ä¾‹+ä¾èµ–æ³¨å…¥ï¼‰ ======================
@singleton
class Neo4jKGRAGService:
    """
    çŸ¥è¯†å›¾è°±RAGæœåŠ¡ï¼ˆé€‚é…é¡¹ç›®ç°æœ‰RAGæ¶æ„ï¼‰
    æ ¸å¿ƒè°ƒæ•´ï¼š
    1. æ‰€æœ‰æ–‡æ¡£ç»Ÿä¸€ä½¿ç”¨å›ºå®šç´¢å¼•ID: kg_rag_index
    2. å¤ç”¨å‘é‡RAGçš„å‘é‡æ•°æ®åº“ï¼Œä¸ºKG-RAGåˆ›å»ºä¸“å±çš„docstore/index_store
    """
    @inject
    def __init__(
        self,
        llm_component: LLMComponent,
        embedding_component: EmbeddingComponent,
        vector_store_component: VectorStoreComponent,
        # ä¿ç•™node_store_componentï¼Œä½†ä»…ä½œä¸ºå‚è€ƒï¼Œä¸å¤ç”¨å…¶å­˜å‚¨
        node_kg_store_component: NodeKgStoreComponent,
        neo4j_config: Neo4jConfig = Neo4jConfig()
    ):
        # å¤ç”¨é¡¹ç›®ç°æœ‰ç»„ä»¶
        self.llm_component = llm_component
        self.embedding_component = embedding_component
        self.node_kg_store_component = node_kg_store_component
        self.vector_store_component = vector_store_component
        self.neo4j_config = neo4j_config
        # 1. åˆå§‹åŒ–Neo4jå›¾è°±å­˜å‚¨ï¼ˆåŸæœ‰é€»è¾‘ï¼Œä¿æŒç‹¬ç«‹ï¼‰
        self.graph_store = self._init_graph_store()

        logger.info(f"âœ… Neo4jå›¾è°±å­˜å‚¨åˆå§‹åŒ–å®Œæˆï¼š{self.neo4j_config}")
        
        # ========== å…³é”®ä¿®å¤ï¼šç¡®ä¿StorageContextå§‹ç»ˆåŒ…å«é»˜è®¤vector_store ==========
        if get_local_kg_data_path().exists():
            # ç›®å½•å­˜åœ¨ä¸”æœ‰æ–‡ä»¶ï¼šä»æœ¬åœ°åŠ è½½StorageContextï¼Œå¹¶å¼ºåˆ¶ç»‘å®švector_store
            logger.info(f"âœ… æ£€æµ‹åˆ°KGæœ¬åœ°å­˜å‚¨ç›®å½•å­˜åœ¨: {get_local_kg_data_path()}ï¼Œå¼€å§‹åŠ è½½æœ¬åœ°ç´¢å¼•")
            self.storage_context = StorageContext.from_defaults(
                persist_dir=get_local_kg_data_path(),  # ä»…æŒ‡å®šæŒä¹…åŒ–ç›®å½•
                graph_store=self.graph_store,
                # å…³é”®ä¿®å¤ï¼šæ˜¾å¼æŒ‡å®šé»˜è®¤vector_store
                vector_store=self.vector_store_component.vector_store
            )
        else:
            # ç›®å½•ä¸å­˜åœ¨/ä¸ºç©ºï¼šé‡æ–°åˆå§‹åŒ–StorageContextï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
            logger.warning(f"âš ï¸ KGæœ¬åœ°å­˜å‚¨ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©º: {get_local_kg_data_path()}ï¼Œé‡æ–°åˆå§‹åŒ–å­˜å‚¨ä¸Šä¸‹æ–‡")
            self.storage_context = StorageContext.from_defaults(
                vector_store=self.vector_store_component.vector_store,
                docstore=self.node_kg_store_component.doc_store,
                index_store=self.node_kg_store_component.index_store,
                graph_store=self.graph_store
            )
        
        # é¢å¤–é˜²æŠ¤ï¼šç¡®ä¿vector_storeså­—å…¸ä¸­æœ‰defaulté”®
        if not hasattr(self.storage_context, 'vector_stores') or 'default' not in self.storage_context.vector_stores:
            logger.warning("âš ï¸ StorageContextç¼ºå°‘default vector_storeï¼Œæ‰‹åŠ¨æ·»åŠ ")
            self.storage_context.vector_stores['default'] = self.vector_store_component.vector_store
            
        logger.info(f"âœ… KGå­˜å‚¨ä¸Šä¸‹æ–‡åˆå§‹åŒ–å®Œæˆ-------------{self.storage_context}")
        # èŠ‚ç‚¹åˆ†å‰²å™¨ï¼ˆä¸åŸæœ‰RAGä½¿ç”¨ç›¸åŒçš„åˆ†å‰²ç­–ç•¥ï¼Œä¿æŒä¸€è‡´ï¼‰
        self.node_parser = SentenceSplitter.from_defaults()
        
        # KGç´¢å¼•å»¶è¿Ÿåˆå§‹åŒ–
        self.kg_index: Optional[KnowledgeGraphIndex] = None

        # åŒé‡æ ¡éªŒç´¢å¼•çŠ¶æ€ï¼ˆNeo4j + æœ¬åœ°æ–‡ä»¶ï¼‰
        self.kg_index_exists = self._check_kg_index_status()
        logger.info(f"âœ… KGç´¢å¼•çŠ¶æ€åŠ è½½å®Œæˆï¼š{'å·²æ„å»º' if self.kg_index_exists else 'æœªæ„å»º'}")
        
        # å¯åŠ¨æ—¶ä¸»åŠ¨åŠ è½½KGç´¢å¼•
        if self.kg_index_exists:
            self._load_kg_index_on_startup()

    def _init_graph_store(self) -> Neo4jGraphStore:
        """åˆå§‹åŒ–Neo4jå›¾è°±å­˜å‚¨ï¼ˆå¼‚å¸¸æ•è·+æ—¥å¿—ï¼ŒåŸæœ‰é€»è¾‘ä¸å˜ï¼‰"""
        try:
            graph_store = Neo4jGraphStore(
                username=self.neo4j_config.username,
                password=self.neo4j_config.password,
                url=self.neo4j_config.url,
                database=self.neo4j_config.database,
            )
            logger.info(f"âœ… æˆåŠŸè¿æ¥Neo4j: {self.neo4j_config.url} (æ•°æ®åº“: {self.neo4j_config.database})")
            return graph_store
        except Exception as e:
            logger.error(f"âŒ Neo4jè¿æ¥å¤±è´¥: {str(e)}", exc_info=True)
            raise ConnectionError(f"Neo4jè¿æ¥å¤±è´¥: {str(e)}")
    
    def _check_kg_index_status(self) -> bool:
        """
        åŒé‡æ ¡éªŒKGç´¢å¼•çŠ¶æ€ï¼š
        1. ä¼˜å…ˆä»Neo4jåŠ è½½çŠ¶æ€
        2. Neo4jçŠ¶æ€ä¸¢å¤±æ—¶ï¼Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨æ–‡ä»¶
        """
        # ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»Neo4jåŠ è½½çŠ¶æ€
        neo4j_status = False
        try:
            neo4j_status = self._load_kg_index_status_from_neo4j()
            if neo4j_status:
                logger.info("âœ… ä»Neo4jæ ¡éªŒåˆ°KGç´¢å¼•å·²æ„å»º")
                return True
        except Exception as e:
            logger.warning(f"âš ï¸ ä»Neo4jæ ¡éªŒç´¢å¼•çŠ¶æ€å¤±è´¥ï¼š{str(e)}")
        
        # ç¬¬äºŒæ­¥ï¼šNeo4jçŠ¶æ€ä¸¢å¤±/æœªæ„å»ºæ—¶ï¼Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨
        local_status = self._check_local_kg_index_files()
        if local_status:
            logger.warning("âš ï¸ Neo4jçŠ¶æ€ä¸¢å¤±ï¼Œä½†æœ¬åœ°å­˜åœ¨ç´¢å¼•æ–‡ä»¶ï¼Œæ ‡è®°ä¸ºå·²æ„å»º")
            # åŒæ­¥çŠ¶æ€åˆ°Neo4j
            self._save_kg_index_status_to_neo4j(True, KG_RAG_INDEX_ID)
            return True
        
        logger.info("â„¹ï¸ æœ¬åœ°å’ŒNeo4jå‡æœªæ£€æµ‹åˆ°KGç´¢å¼•ï¼Œæ ‡è®°ä¸ºæœªæ„å»º")
        return False
    
    def _check_local_kg_index_files(self) -> bool:
        """
        ä¿®å¤ç‰ˆï¼šä¸å†æ£€æŸ¥å›ºå®šç´¢å¼•IDï¼Œä»…æ£€æŸ¥æ˜¯å¦æœ‰KGç´¢å¼•æ–‡ä»¶å­˜åœ¨
        """
        kg_path = get_local_kg_data_path()
        if not kg_path.exists():
            return False
        
        # æ£€æŸ¥å…³é”®ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = [
            kg_path / "index_store.json",
            kg_path / "docstore.json",
            kg_path / "vector_store.json"
        ]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªå…³é”®æ–‡ä»¶å­˜åœ¨ä¸”éç©º
        for file_path in required_files:
            if file_path.exists() and file_path.stat().st_size > 0:
                return True
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦æœ‰å…¶ä»–ç´¢å¼•ç›¸å…³æ–‡ä»¶
        all_files = list(kg_path.glob("*"))
        if len(all_files) > 0:
            return True
        
        return False
    
    def _load_kg_index_status_from_neo4j(self) -> bool:
        """ä»Neo4jåŠ è½½KGç´¢å¼•çŠ¶æ€ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰"""
        if not self.graph_store:
            logger.warning("âš ï¸ Neo4jæœªåˆå§‹åŒ–ï¼Œé»˜è®¤KGç´¢å¼•æœªæ„å»º")
            return False
        
        try:
            load_status_query = "MATCH (n:KGIndexStatus) RETURN n.exists as exists"
            query_results = self.graph_store.query(load_status_query)
            
            if query_results and len(query_results) > 0:
                return query_results[0]["exists"]
            
            logger.warning("âš ï¸ Neo4jä¸­æœªæ‰¾åˆ°KGç´¢å¼•çŠ¶æ€èŠ‚ç‚¹ï¼Œé»˜è®¤ç´¢å¼•æœªæ„å»º")
            return False
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½KGç´¢å¼•çŠ¶æ€å¤±è´¥ï¼Œé»˜è®¤ç´¢å¼•æœªæ„å»ºï¼š{str(e)}")
            return False
    
    def _save_kg_index_status_to_neo4j(self, exists: bool, index_id: str = KG_RAG_INDEX_ID):
        """å°†KGç´¢å¼•çŠ¶æ€æŒä¹…åŒ–åˆ°Neo4jï¼ˆä¿®å¤Cypherè¯­æ³•é”™è¯¯ï¼‰"""
        if not self.graph_store:
            logger.warning("âš ï¸ Neo4jæœªåˆå§‹åŒ–ï¼Œè·³è¿‡KGç´¢å¼•çŠ¶æ€ä¿å­˜")
            return
        
        max_retries = 3  # å¢åŠ é‡è¯•æœºåˆ¶
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # å…ˆåˆ é™¤åŸæœ‰çŠ¶æ€èŠ‚ç‚¹ï¼ˆä¿è¯å”¯ä¸€æ€§ï¼‰
                delete_status_query = "MATCH (n:KGIndexStatus) DELETE n"
                self.graph_store.query(delete_status_query)
                
                # ç®€åŒ–Cypherè¯­å¥ï¼Œç§»é™¤å¤šä½™ç¼©è¿›å’Œæ¢è¡Œ
                create_status_query = """
CREATE (n:KGIndexStatus {exists: $exists, update_time: $update_time, node_desc: "KGç´¢å¼•çŠ¶æ€æ ‡è®°èŠ‚ç‚¹ï¼Œè¯·å‹¿æ‰‹åŠ¨åˆ é™¤", database: $database, version: "1.0", index_id: $index_id})
                """.strip()  # å»é™¤é¦–å°¾ç©ºç™½
                
                self.graph_store.query(
                    create_status_query,
                    {
                        "exists": exists,
                        "update_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "database": self.neo4j_config.database,
                        "index_id": index_id  # å­˜å‚¨å›ºå®šç´¢å¼•ID
                    }
                )
                
                self.kg_index_exists = exists
                logger.info(f"âœ… KGç´¢å¼•çŠ¶æ€å·²æŒä¹…åŒ–åˆ°Neo4jï¼š{'å·²æ„å»º' if exists else 'æœªæ„å»º'} (ç´¢å¼•ID: {KG_RAG_INDEX_ID})")
                return
            except Exception as e:
                retry_count += 1
                logger.error(f"âŒ ä¿å­˜KGç´¢å¼•çŠ¶æ€åˆ°Neo4jå¤±è´¥(é‡è¯•{retry_count}/{max_retries}): {str(e)}")
                if retry_count >= max_retries:
                    logger.error(f"âŒ ä¿å­˜KGç´¢å¼•çŠ¶æ€åˆ°Neo4jæœ€ç»ˆå¤±è´¥ï¼Œå°†å°è¯•æœ¬åœ°æ–‡ä»¶æ ‡è®°")
                    # æœ¬åœ°æ–‡ä»¶æ ‡è®°ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
                    self._save_kg_index_status_locally(exists)
                    raise
    
    def _save_kg_index_status_locally(self, exists: bool):
        """æœ¬åœ°æ–‡ä»¶ä¿å­˜ç´¢å¼•çŠ¶æ€ï¼ˆNeo4jå¤±è´¥æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        try:
            status_file = get_local_kg_data_path() / "kg_index_status.json"
            status_file.parent.mkdir(exist_ok=True, parents=True)
            
            import json
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "exists": exists,
                    "index_id": KG_RAG_INDEX_ID,  # å­˜å‚¨å›ºå®šç´¢å¼•ID
                    "update_time": datetime.datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… KGç´¢å¼•çŠ¶æ€å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼š{status_file} (ç´¢å¼•ID: {KG_RAG_INDEX_ID})")
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°ä¿å­˜KGç´¢å¼•çŠ¶æ€ä¹Ÿå¤±è´¥ï¼š{str(e)}")
    
    def _load_kg_index_on_startup(self) -> None:
        """
        ä¿®å¤ç‰ˆï¼šå¯åŠ¨æ—¶åŠ è½½KGç´¢å¼•ï¼ˆè‡ªåŠ¨è¯†åˆ«UUIDç´¢å¼•IDï¼Œä¸å†ä¾èµ–è‡ªå®šä¹‰kg_rag_indexï¼‰
        """
        try:
            logger.info("ğŸ”„ å¯åŠ¨æ—¶ä¸»åŠ¨åŠ è½½KGç´¢å¼•ï¼ˆè‡ªåŠ¨è¯†åˆ«UUIDç´¢å¼•IDï¼‰...")
            
            # ========== æ ¸å¿ƒä¿®å¤1ï¼šå…ˆè¯»å–index_store.jsonï¼Œæ‰¾åˆ°KGç±»å‹çš„ç´¢å¼•UUID ==========
            kg_path = get_local_kg_data_path()
            index_store_file = kg_path / "index_store.json"
            
            target_index_id = None
            if index_store_file.exists():
                import json
                with open(index_store_file, 'r', encoding='utf-8') as f:
                    index_store_data = json.load(f)
                
                # éå†æ‰€æœ‰ç´¢å¼•ï¼Œæ‰¾åˆ°KGç±»å‹çš„ç´¢å¼•ï¼ˆ__type__ == "kg"ï¼‰
                for idx_id, idx_data in index_store_data.get("index_store/data", {}).items():
                    if idx_data.get("__type__") == "kg":
                        target_index_id = idx_id
                        logger.info(f"âœ… æ‰¾åˆ°KGç±»å‹çš„ç´¢å¼•UUID: {target_index_id}")
                        break
            
            # ========== æ ¸å¿ƒä¿®å¤2ï¼šæ ¹æ®æ‰¾åˆ°çš„UUIDåŠ è½½ç´¢å¼• ==========
            if target_index_id:
                self.kg_index = load_index_from_storage(
                    storage_context=self.storage_context,
                    index_cls=KnowledgeGraphIndex,
                    index_id=target_index_id  # ä½¿ç”¨æ¡†æ¶ç”Ÿæˆçš„UUID
                )
                # æ¢å¤ç´¢å¼•çš„ä¾èµ–ç»„ä»¶
                self.kg_index._llm = self.llm_component.llm
                self.kg_index._embed_model = self.embedding_component.embedding_model
                self.kg_index._graph_store = self.graph_store
                self.kg_index._node_parser = self.node_parser
                logger.info(f"âœ… å¯åŠ¨æ—¶æˆåŠŸåŠ è½½KGç´¢å¼•ï¼ˆUUID: {target_index_id}ï¼‰")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°KGç±»å‹çš„ç´¢å¼•ï¼Œç´¢å¼•å¯èƒ½å°šæœªæ„å»º")
                self.kg_index = None
            
            # ========== æ ¸å¿ƒä¿®å¤3ï¼šæ›´æ–°ç´¢å¼•çŠ¶æ€ï¼ˆåŸºäºå®é™…æ˜¯å¦åŠ è½½æˆåŠŸï¼‰ ==========
            if self.kg_index is not None:
                self.kg_index_exists = True
                logger.info("âœ… KGç´¢å¼•çŠ¶æ€åŠ è½½å®Œæˆï¼šå·²æ„å»º")
                # æŒä¹…åŒ–çŠ¶æ€åˆ°Neo4jï¼ˆä½¿ç”¨ä¸šåŠ¡ç´¢å¼•IDæ ‡è¯†ï¼‰
                self._save_kg_index_status_to_neo4j(True, KG_RAG_INDEX_ID)
            else:
                self.kg_index_exists = False
                logger.info("âœ… KGç´¢å¼•çŠ¶æ€åŠ è½½å®Œæˆï¼šæœªæ„å»º")
                self._save_kg_index_status_to_neo4j(False, KG_RAG_INDEX_ID)
                
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨æ—¶åŠ è½½KGç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
            # å¤±è´¥åä»æ ‡è®°ç´¢å¼•çŠ¶æ€ä¸ºæœªæ„å»ºï¼Œä½†ä¿ç•™docstoreä¸­çš„æ–‡æ¡£æ•°æ®
            self.kg_index = None
            self.kg_index_exists = False
            self._save_kg_index_status_to_neo4j(False, KG_RAG_INDEX_ID)                          

    # ====================== æ¸…ç†Neo4jä¸­çš„æ— æ•ˆä¸‰å…ƒç»„ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰ ======================
    def _clean_invalid_triples_in_neo4j(self):
        if not self.graph_store:
            logger.warning("âš ï¸ Neo4jæœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ— æ•ˆä¸‰å…ƒç»„æ¸…ç†")
            return
        
        # å®šä¹‰æ— æ•ˆå…³é”®è¯ï¼ˆä¸æ–‡æœ¬æ¸…ç†é€»è¾‘å¯¹é½ï¼‰
        invalid_keywords = ['E:', 'Tmp', 'tmp', '.txt', 'backend_app', 'llama3.2-projec', 'Backend_app', 'Ai']
        
        try:
            # è½¬ä¹‰å…³é”®è¯ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            escaped_keywords = [keyword.replace("'", "\\'").replace('"', '\\"') for keyword in invalid_keywords]
            keywords_str = ", ".join([f"'{kw}'" for kw in escaped_keywords])
            
            # æŸ¥è¯¢æ‰€æœ‰åŒ…å«æ— æ•ˆå…³é”®è¯çš„èŠ‚ç‚¹
            invalid_node_query = f"""
            MATCH (n) 
            WHERE ANY(keyword IN [{keywords_str}] WHERE 
                ANY(prop IN keys(n) WHERE 
                    toLower(toString(n[prop])) CONTAINS toLower(keyword)
                )
            )
            RETURN elementId(n) as node_id
            """
            invalid_nodes = self.graph_store.query(invalid_node_query)
            
            if not invalid_nodes:
                logger.info("âœ… Neo4jä¸­æ— æ— æ•ˆä¸‰å…ƒç»„ï¼Œæ— éœ€æ¸…ç†")
                return
            
            # åˆ é™¤è¿™äº›æ— æ•ˆèŠ‚ç‚¹åŠå…¶å…³è”çš„å…³ç³»
            delete_invalid_query = f"""
            MATCH (n) 
            WHERE ANY(keyword IN [{keywords_str}] WHERE 
                ANY(prop IN keys(n) WHERE 
                    toLower(toString(n[prop])) CONTAINS toLower(keyword)
                )
            )
            DETACH DELETE n
            """
            self.graph_store.query(delete_invalid_query)
            
            # éªŒè¯æ¸…ç†ç»“æœ
            remaining_triples = self.graph_store.query("MATCH (s)-[r]->(o) RETURN count(*) as total")
            logger.info(f"âœ… æˆåŠŸæ¸…ç†Neo4jä¸­çš„æ— æ•ˆä¸‰å…ƒç»„ï¼š")
            logger.info(f"   - æ¸…ç†çš„æ— æ•ˆèŠ‚ç‚¹æ•°é‡ï¼š{len(invalid_nodes)}")
            logger.info(f"   - æ¸…ç†åå‰©ä½™æœ‰æ•ˆä¸‰å…ƒç»„æ•°é‡ï¼š{remaining_triples[0]['total'] if remaining_triples else 0}")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†Neo4jæ— æ•ˆä¸‰å…ƒç»„å¤±è´¥: {str(e)}", exc_info=True)

    # ====================== æ–‡æ¡£å¤„ç†ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šç»‘å®šå›ºå®šç´¢å¼•IDï¼‰ ======================
    def _ingest_data(self, file_name: str, file_data: AnyStr) -> list[IngestedDoc]:
        PROJECT_TMP_DIR = Path(__file__).parent.parent.parent.parent / "tmp"
        PROJECT_TMP_DIR.mkdir(exist_ok=True, mode=0o777)
        path_to_tmp = None

        try:
            with tempfile.NamedTemporaryFile(
                dir=str(PROJECT_TMP_DIR),
                suffix=Path(file_name).suffix,
                delete=False
            ) as tmp:
                path_to_tmp = Path(tmp.name)
                if isinstance(file_data, bytes):
                    tmp.write(file_data)
                else:
                    tmp.write(str(file_data).encode("utf-8"))
                tmp.flush()
                os.fsync(tmp.fileno())

            return self.ingest_file(file_name, path_to_tmp)
        finally:
            if path_to_tmp and path_to_tmp.exists():
                try:
                    time.sleep(0.5)
                    path_to_tmp.unlink()
                    logger.debug(f"âœ… ä¸´æ—¶æ–‡ä»¶ {path_to_tmp} å·²æˆåŠŸæ¸…ç†")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼š{str(e)}ï¼Œæ–‡ä»¶å°†æ®‹ç•™ï¼Œå»ºè®®åç»­å®šæ—¶æ¸…ç†")

    def _clean_document_text(self, text: str) -> str:
        if not text:
            return ""
        
        # è¿‡æ»¤è·¯å¾„æ¨¡å¼
        text = re.sub(r'[A-Za-z]:(\\|/)?[^\\/\n]*', '', text)
        text = re.sub(r'^[A-Za-z]:$', '', text, flags=re.MULTILINE)
        
        # è¿‡æ»¤ä¸´æ—¶æ–‡ä»¶å
        text = re.sub(r'Tmp\w+\.txt', '', text)
        
        # è¿‡æ»¤é¡¹ç›®å…³é”®è¯
        project_keywords = r'\b(tmp|Tmp|TEMP|temp|Backend_app|Ai)\b'
        text = re.sub(project_keywords, '', text, flags=re.IGNORECASE)
        
        # è¿‡æ»¤å¤šä½™ç©ºæ ¼å’Œç©ºè¡Œ
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def ingest_file(self, file_name: str, file_data: Path) -> list[IngestedDoc]:
        # 1. åŠ è½½æ–‡æ¡£
        from llama_index.core import SimpleDirectoryReader
        documents = SimpleDirectoryReader(input_files=[file_data]).load_data()
        logger.info(f"åŠ è½½æ–‡ä»¶ {file_name} å®Œæˆï¼ŒåŸå§‹æ–‡æ¡£å—æ•°é‡ï¼š{len(documents)}")

        # 2. æ–‡æ¡£å†…å®¹é¢„å¤„ç†
        processed_docs = []
        for doc in documents:
            clean_text = self._clean_document_text(doc.text)
            if clean_text:
                processed_doc = LlamaDoc(
                    text=clean_text,
                    metadata=doc.metadata,
                    id_=doc.id_
                )
                processed_doc.metadata["original_file_name"] = file_name
                processed_doc.metadata["index_id"] = KG_RAG_INDEX_ID  # æ ‡è®°æ–‡æ¡£æ‰€å±ç´¢å¼•
                processed_docs.append(processed_doc)
        logger.info(f"æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æ¡£å—æ•°é‡ï¼š{len(processed_docs)}")

        # 3. æ¸…ç©ºå†å²æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if settings().neo4j.clear_existing_data:
            self.clear_neo4j_data()
            logger.info("âœ… å·²æ¸…ç©ºNeo4jç°æœ‰å›¾è°±æ•°æ®")

        # ========== é¢å¤–é˜²æŠ¤ï¼šå†æ¬¡ç¡®è®¤StorageContextçš„vector_store ==========
        if not hasattr(self.storage_context, 'vector_stores') or 'default' not in self.storage_context.vector_stores:
            self.storage_context.vector_stores['default'] = self.vector_store_component.vector_store
        
        # 4. æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•ï¼ˆå¤ç”¨å‘é‡åº“ï¼Œå­˜å‚¨åˆ°KGä¸“å±å­˜å‚¨ï¼ŒæŒ‡å®šå›ºå®šç´¢å¼•IDï¼‰
        if self.kg_index is None:
            # é¦–æ¬¡æ„å»ºï¼šåˆ›å»ºæ–°ç´¢å¼•å¹¶æŒ‡å®šå›ºå®šID
            self.kg_index = KnowledgeGraphIndex.from_documents(
                documents=processed_docs,
                storage_context=self.storage_context,
                max_triplets_per_chunk=self.neo4j_config.max_triplets_per_chunk,
                include_embeddings=self.neo4j_config.include_embeddings,
                embed_model=self.embedding_component.embedding_model,
                llm=self.llm_component.llm,
                node_parser=self.node_parser,
                index_id=KG_RAG_INDEX_ID,  # å…³é”®ï¼šæŒ‡å®šå›ºå®šç´¢å¼•ID
                # ä¸‰å…ƒç»„æå–æç¤ºï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
                kg_triple_extract_template="""
                # ä»»åŠ¡è¦æ±‚
                ä»ä»¥ä¸‹æ–‡æœ¬ä¸­ä»…æå–**ä¸šåŠ¡å†…å®¹ç›¸å…³**çš„ä¸‰å…ƒç»„ï¼ˆä¸»ä½“ï¼Œå…³ç³»ï¼Œå®¢ä½“ï¼‰ï¼Œä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

                # è¿‡æ»¤è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰
                1. å®Œå…¨å¿½ç•¥ä»»ä½•ä¸æ–‡ä»¶ç³»ç»Ÿç›¸å…³çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
                - æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ï¼šE:\ã€/home/userã€C:/ï¼‰
                - æ–‡ä»¶åï¼ˆå¦‚ï¼šdocument.txtã€image.pngï¼‰
                - ç›®å½•åï¼ˆå¦‚ï¼štmpã€Backend_appã€Aiï¼‰
                - ç›˜ç¬¦ï¼ˆå¦‚ï¼šC:ã€D:ï¼‰
                2. åªæå–æ–‡æœ¬ä¸­æè¿°å®ä½“ã€å±æ€§ã€å…³ç³»çš„æœ‰æ•ˆä¿¡æ¯ã€‚
                3. ä¸»ä½“å’Œå®¢ä½“å¿…é¡»æ˜¯æœ‰å®é™…ä¸šåŠ¡å«ä¹‰çš„åè¯/çŸ­è¯­ï¼Œå…³ç³»å¿…é¡»æ˜¯èƒ½ä½“ç°ä¸¤è€…å…³è”çš„åŠ¨è¯/ä»‹è¯çŸ­è¯­ã€‚

                # å¥½çš„ç¤ºä¾‹
                - ("Python", "æ˜¯ä¸€ç§", "ç¼–ç¨‹è¯­è¨€")
                - ("ç‰›é¡¿", "æå‡ºäº†", "ä¸‡æœ‰å¼•åŠ›å®šå¾‹")
                - ("ã€Šä¸‰ä½“ã€‹", "çš„ä½œè€…æ˜¯", "åˆ˜æ…ˆæ¬£")

                # åçš„ç¤ºä¾‹ï¼ˆè¯·ä¸è¦è¾“å‡ºè¿™æ ·çš„å†…å®¹ï¼‰
                - ("E:", "IS_LOCATED_IN", "Ai")
                - ("Tmpfile.txt", "HAS_CONTENT", "data")

                # è¾“å‡ºæ ¼å¼ï¼ˆä»…è¿”å›åˆ—è¡¨ï¼Œæ— å…¶ä»–æ–‡å­—ï¼‰
                [("ä¸»ä½“1", "å…³ç³»1", "å®¢ä½“1"), ("ä¸»ä½“2", "å…³ç³»2", "å®¢ä½“2")]

                # éœ€è¦æå–çš„æ–‡æœ¬
                {text}
                """ 
            )

            try:
                # æ–¹æ¡ˆ1ï¼šç›´æ¥è°ƒç”¨index_storeçš„set_index_metadataï¼ˆæ— éœ€å¯¼å…¥ç±»ï¼‰
                # ä¸ç®¡åº•å±‚å®ç°æ˜¯ä»€ä¹ˆï¼Œç›´æ¥è°ƒç”¨æ–¹æ³•å³å¯
                self.storage_context.index_store.set_index_metadata(
                    KG_RAG_INDEX_ID,
                    {
                        "type": "knowledge_graph", 
                        "version": "1.0",
                        "created_at": datetime.datetime.now().isoformat()
                    }
                )
                logger.info(f"å·²å°†ç´¢å¼•ID {KG_RAG_INDEX_ID} å†™å…¥index_store")
            except Exception as e:
                logger.warning(f"å†™å…¥ç´¢å¼•å…ƒæ•°æ®å¤±è´¥ï¼ˆä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ï¼‰: {str(e)}") 
        else:
            # å¢é‡æ·»åŠ ï¼šå‘å·²æœ‰ç´¢å¼•ä¸­æ·»åŠ æ–‡æ¡£
            logger.info(f"ğŸ“„ å‘å·²æœ‰KGç´¢å¼•ï¼ˆ{KG_RAG_INDEX_ID}ï¼‰å¢é‡æ·»åŠ æ–‡æ¡£")
            self.kg_index.insert_nodes(
                nodes=self.node_parser.get_nodes_from_documents(processed_docs),
                max_triplets_per_chunk=self.neo4j_config.max_triplets_per_chunk,
                include_embeddings=self.neo4j_config.include_embeddings
            )
        
        self.storage_context.persist(persist_dir=get_local_kg_data_path())
        # å¯ç”¨æ— æ•ˆä¸‰å…ƒç»„æ¸…ç†ï¼ˆåŸæœ‰æ³¨é‡Šå–æ¶ˆï¼‰
        #self._clean_invalid_triples_in_neo4j()
        
        # å¼ºåˆ¶æ›´æ–°ç´¢å¼•çŠ¶æ€
        self.kg_index_exists = True
        self._save_kg_index_status_to_neo4j(True, KG_RAG_INDEX_ID)
        
        # 5. ä»Neo4jä¸­è·å–ä¸‰å…ƒç»„å¹¶è¿‡æ»¤æ— æ•ˆæ•°æ®
        try:
            cypher_query = """MATCH (s)-[r]->(o) RETURN s.id AS subject, type(r) AS relation, o.id AS object"""
            query_results = self.graph_store.query(cypher_query)
            all_triples = [
                (result["subject"], result["relation"], result["object"]) 
                for result in query_results
            ]
            
            # åè¿‡æ»¤æ— æ•ˆä¸‰å…ƒç»„
            valid_triples = []
            invalid_keywords = ['E:', 'Tmp', 'tmp', '.txt', 'backend_app', 'llama3.2-projec', 'Backend_app', 'Ai']
            for triple in all_triples:
                if not any(keyword in str(triple) for keyword in invalid_keywords):
                    valid_triples.append(triple)
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±ç´¢å¼•æ„å»ºå®Œæˆï¼ˆç´¢å¼•ID: {KG_RAG_INDEX_ID}ï¼‰ï¼š")
            logger.info(f"   - åŸå§‹ä¸‰å…ƒç»„æ•°é‡ï¼š{len(all_triples)}")
            logger.info(f"   - æœ‰æ•ˆä¸‰å…ƒç»„æ•°é‡ï¼š{len(valid_triples)}")
            logger.info(f"   - æœ‰æ•ˆä¸‰å…ƒç»„å†…å®¹ï¼š{valid_triples if valid_triples else 'æ— æœ‰æ•ˆä¸‰å…ƒç»„'}")
            
        except Exception as e:
            logger.error(f"è·å–Neo4jä¸‰å…ƒç»„æ•°é‡å¤±è´¥: {str(e)}", exc_info=True)
            logger.warning(f"âš ï¸ æ— æ³•è·å–ä¸‰å…ƒç»„æ•°é‡ï¼Œå·²é™çº§ä¸º0ï¼ˆæ–‡ä»¶ï¼š{file_name}ï¼‰")

        # 6. æ˜ å°„ä¸ºé¡¹ç›®ç»Ÿä¸€çš„IngestedDocæ¨¡å‹
        current_ingested_docs = [IngestedDoc.from_document(doc) for doc in processed_docs]
        
        # 7. æŸ¥è¯¢KGä¸“å±å­˜å‚¨ä¸­æ‰€æœ‰å·²å…¥åº“çš„å…¨é‡æ–‡æ¡£
        all_ingested_docs = self.list_ingested_kg_docs()
        
        logger.info(f"âœ… å½“å‰ä¸Šä¼ æ–‡æ¡£æ•°ï¼š{len(current_ingested_docs)}ï¼ŒKGä¸“å±å­˜å‚¨å…¨é‡æ–‡æ¡£æ•°ï¼š{len(all_ingested_docs)}")
        return all_ingested_docs

    def ingest_bin_data(self, file_name: str, raw_file_data: BinaryIO) -> list[IngestedDoc]:
        """å¤„ç†äºŒè¿›åˆ¶æ–‡ä»¶æµï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰"""
        try:
            raw_file_data.seek(0)
            file_data = raw_file_data.read()
            return self._ingest_data(file_name, file_data)
        except Exception as e:
            logger.error(f"å¤„ç†äºŒè¿›åˆ¶æ–‡ä»¶ {file_name} å¤±è´¥: {str(e)}", exc_info=True)
            raise

    # ====================== çŸ¥è¯†å›¾è°±RAGæŸ¥è¯¢ï¼ˆä¼˜åŒ–åŠ è½½é€»è¾‘ï¼‰ ======================
    def get_kg_query_engine(self,** kwargs) -> "QueryEngine":
        # å†æ¬¡æ ¡éªŒæœ¬åœ°æ–‡ä»¶
        if not self.kg_index_exists:
            # é‡æ–°æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
            if self._check_local_kg_index_files():
                self.kg_index_exists = True
                self._save_kg_index_status_to_neo4j(True, KG_RAG_INDEX_ID)
            else:
                raise RuntimeError(f"çŸ¥è¯†å›¾è°±ç´¢å¼•ï¼ˆä¸šåŠ¡ID: {KG_RAG_INDEX_ID}ï¼‰æœªæ„å»ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
        # åŒé‡æ£€æŸ¥å¹¶é‡æ–°åŠ è½½ç´¢å¼•
        if not self.kg_index:
            logger.warning(f"âš ï¸ kg_indexä¸ºç©ºï¼Œå°è¯•é‡æ–°åŠ è½½å›ºå®šç´¢å¼• {KG_RAG_INDEX_ID}...")
            self._load_kg_index_on_startup()  # å¤ç”¨å¯åŠ¨åŠ è½½é€»è¾‘
            if not self.kg_index:
                raise RuntimeError(f"çŸ¥è¯†å›¾è°±ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡æ¡£")
 
        # é»˜è®¤é…ç½®ï¼ˆå¯é€šè¿‡kwargsè¦†ç›–ï¼‰
        query_config = {
            "include_text": kwargs.get("include_text", True),
            "response_mode": kwargs.get("response_mode", "tree_summarize"),
            "embedding_mode": kwargs.get("embedding_mode", "hybrid"),
            "similarity_top_k": kwargs.get("similarity_top_k", 5),
            "llm": self.llm_component.llm,
            "embed_model": self.embedding_component.embedding_model
        }

        return self.kg_index.as_query_engine(** query_config)

    def query_kg_rag(self, query_text: str, **kwargs) -> str:
        """æ‰§è¡ŒçŸ¥è¯†å›¾è°±RAGæŸ¥è¯¢"""
        try:
            query_engine = self.get_kg_query_engine(** kwargs)
            response = query_engine.query(query_text)
            return str(response)
        except Exception as e:
            logger.error(f"KG RAGæŸ¥è¯¢å¤±è´¥(ç´¢å¼•ID: {KG_RAG_INDEX_ID}): {str(e)}", exc_info=True)
            raise

    # ====================== è¾…åŠ©æ–¹æ³•ï¼ˆé€‚é…KGä¸“å±å­˜å‚¨ï¼‰ ======================
    def clear_neo4j_data(self) -> None:
        """æ¸…ç©ºNeo4jæ‰€æœ‰èŠ‚ç‚¹/å…³ç³»åŠKGä¸“å±å­˜å‚¨æ•°æ®"""
        if not self.graph_store:
            raise RuntimeError("Neo4jå›¾è°±å­˜å‚¨æœªåˆå§‹åŒ–")
        # æ¸…ç©ºNeo4jå›¾æ•°æ®
        self.graph_store.query("MATCH (n) DETACH DELETE n")
        # æ¸…ç©ºKGä¸“å±æ–‡æ¡£å­˜å‚¨å’Œç´¢å¼•å­˜å‚¨
        self.node_kg_store_component.doc_store.clear()
        self.node_kg_store_component.index_store.clear()
        # é‡ç½®KGç´¢å¼•
        self.kg_index = None
        # åŒæ­¥çŠ¶æ€åˆ°Neo4j
        self.kg_index_exists = False
        self._save_kg_index_status_to_neo4j(False, KG_RAG_INDEX_ID)
        logger.warning(f"âš ï¸ Neo4jæ‰€æœ‰æ•°æ®åŠKGä¸“å±å­˜å‚¨æ•°æ®å·²æ¸…ç©ºï¼ˆç´¢å¼•ID: {KG_RAG_INDEX_ID}ï¼‰")

    def list_ingested_kg_docs(self) -> list[IngestedDoc]:
        """
        ä¼˜åŒ–ç‰ˆï¼šç›´æ¥è¯»å–docstore.jsonè·å–æ–‡æ¡£åˆ—è¡¨ï¼ˆä¸ä¾èµ–kg_indexï¼‰
        """
        try:
            kg_path = get_local_kg_data_path()
            docstore_file = kg_path / "docstore.json"
            
            if not docstore_file.exists():
                logger.info("docstore.jsonä¸å­˜åœ¨ï¼Œè¿”å›ç©ºæ–‡æ¡£åˆ—è¡¨")
                return []
            
            import json
            with open(docstore_file, 'r', encoding='utf-8') as f:
                docstore_data = json.load(f)
            
            ingested_docs = []
            
            # éå†ref_doc_infoï¼Œç­›é€‰index_id=kg_rag_indexçš„æ–‡æ¡£
            for doc_id, doc_info in docstore_data.get('docstore/ref_doc_info', {}).items():
                metadata = doc_info.get('metadata', {})
                # åªè¿”å›å½’å±kg_rag_indexçš„æ–‡æ¡£
                if metadata.get('index_id') == KG_RAG_INDEX_ID:
                    ingested_docs.append(
                        IngestedDoc(
                            object="ingest.kg_document",
                            doc_id=doc_id,
                            doc_metadata=metadata
                        )
                    )
            
            logger.info(f"âœ… ä»docstore.jsonè¯»å–åˆ°KGæ–‡æ¡£åˆ—è¡¨: {len(ingested_docs)} ä¸ª")
            return ingested_docs
            
        except Exception as e:
            logger.error(f"è·å–KGæ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
            return []

    def delete_kg_doc(self, doc_id: str) -> None:
        """
        çœŸæ­£åˆ é™¤æŒ‡å®šIDçš„KGæ–‡æ¡£ï¼ˆä¿®æ”¹æŒä¹…åŒ–æ–‡ä»¶+æ¸…ç†å…³è”æ•°æ®ï¼‰
        """
        try:
            logger.info(f"å¼€å§‹åˆ é™¤KGæ–‡æ¡£(ç´¢å¼•ID: {KG_RAG_INDEX_ID}): {doc_id}")
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿kg_indexå·²åˆå§‹åŒ–
            if self.kg_index is None:
                logger.warning(f"KGç´¢å¼•æœªåˆå§‹åŒ–ï¼Œå°è¯•åŠ è½½å›ºå®šç´¢å¼• {KG_RAG_INDEX_ID} åå†åˆ é™¤")
                self._load_kg_index_on_startup()
                if not self.kg_index:
                    raise RuntimeError("KGç´¢å¼•åŠ è½½å¤±è´¥ï¼Œæ— æ³•åˆ é™¤æ–‡æ¡£")
            
            # ========== å…³é”®ä¿®å¤1ï¼šå…ˆæ‰¾åˆ°åŸå§‹æ–‡æ¡£å…³è”çš„æ‰€æœ‰èŠ‚ç‚¹ID ==========
            node_ids_to_delete = []
            # 1. ä» ref_doc_info ä¸­è·å–è¯¥æ–‡æ¡£å…³è”çš„èŠ‚ç‚¹ID
            if doc_id in self.kg_index.ref_doc_info:
                node_ids_to_delete = self.kg_index.ref_doc_info[doc_id].node_ids
                # åˆ é™¤ ref_doc_info ä¸­çš„è®°å½•
                del self.kg_index.ref_doc_info[doc_id]
                logger.info(f"å·²åˆ é™¤ ref_doc_info ä¸­æ–‡æ¡£ {doc_id} çš„è®°å½•")
            
            # ========== å…³é”®ä¿®å¤2ï¼šåˆ é™¤ docstore ä¸­çš„èŠ‚ç‚¹æ•°æ®ï¼ˆå†…å­˜ä¸­ï¼‰ ==========
            for node_id in node_ids_to_delete:
                if node_id in self.kg_index.docstore.docs:
                    del self.kg_index.docstore.docs[node_id]
                    logger.info(f"å·²åˆ é™¤ docstore ä¸­èŠ‚ç‚¹ {node_id} çš„è®°å½•")
            
            # ========== å…³é”®ä¿®å¤3ï¼šåˆ é™¤ metadata ä¸­çš„å…³è”è®°å½• ==========
            # éå†å¹¶åˆ é™¤è¯¥æ–‡æ¡£/èŠ‚ç‚¹çš„ metadata è®°å½•
            metadata_keys_to_delete = []
            if hasattr(self.kg_index.docstore, '_metadata'):
                for key in self.kg_index.docstore._metadata.keys():
                    # åˆ é™¤åŸå§‹æ–‡æ¡£çš„ metadata
                    if key == doc_id:
                        metadata_keys_to_delete.append(key)
                    # åˆ é™¤èŠ‚ç‚¹çš„ metadata
                    elif key in node_ids_to_delete:
                        metadata_keys_to_delete.append(key)
                
                for key in metadata_keys_to_delete:
                    del self.kg_index.docstore._metadata[key]
                    logger.info(f"å·²åˆ é™¤ metadata ä¸­ {key} çš„è®°å½•")
            
            # ========== å…³é”®ä¿®å¤4ï¼šå¼ºåˆ¶é‡æ–°æŒä¹…åŒ– storage_context ==========
            # æ¸…ç©ºåŸæœ‰æŒä¹…åŒ–æ–‡ä»¶ï¼ˆå…³é”®ï¼å¦åˆ™æ—§æ•°æ®ä¼šæ®‹ç•™ï¼‰
            kg_path = get_local_kg_data_path()
            docstore_file = kg_path / "docstore.json"
            if docstore_file.exists():
                docstore_file.unlink()  # åˆ é™¤åŸæœ‰æ–‡ä»¶
                logger.info(f"å·²åˆ é™¤åŸæœ‰ docstore.json æ–‡ä»¶")
            
            # é‡æ–°æŒä¹…åŒ–ï¼ˆç”Ÿæˆæ–°çš„ docstore.jsonï¼‰
            self.storage_context.persist(persist_dir=kg_path)
            logger.info(f"å·²é‡æ–°æŒä¹…åŒ– storage_contextï¼Œdocstore.json å·²æ›´æ–°")
            
            # ========== è¡¥å……ï¼šå°è¯•åˆ é™¤ Neo4j ä¸­å…³è”çš„ä¸‰å…ƒç»„ï¼ˆåŸºäºæ–‡æœ¬å†…å®¹åŒ¹é…ï¼‰ ==========
            # æ³¨æ„ï¼šè¿™æ˜¯è¿‘ä¼¼åˆ é™¤ï¼Œå› ä¸ºä¸‰å…ƒç»„å’Œæ–‡æ¡£æ²¡æœ‰å¼ºç»‘å®š
            try:
                # ä» docstore ä¸­è·å–åŸå§‹æ–‡æ¡£æ–‡æœ¬ï¼ˆå¦‚æœè¿˜èƒ½æ‹¿åˆ°ï¼‰
                if hasattr(self.kg_index.docstore, 'get_document'):
                    try:
                        doc = self.kg_index.docstore.get_document(doc_id)
                        if doc and doc.text:
                            # ç®€å•åŒ¹é…ï¼šåˆ é™¤åŒ…å«æ–‡æ¡£ç‰¹å¾æ–‡æœ¬çš„èŠ‚ç‚¹
                            # æ³¨æ„ï¼šè¿™æ˜¯è¿‘ä¼¼åŒ¹é…ï¼Œå¯èƒ½è¯¯åˆ ï¼Œç”Ÿäº§ç¯å¢ƒéœ€æ›´ç²¾å‡†çš„ç­–ç•¥
                            clean_text = self._clean_document_text(doc.text)
                            # æå–æ–‡æ¡£ä¸­çš„æ ¸å¿ƒå®ä½“
                            entities = re.findall(r'[\u4e00-\u9fa5]{2,}|[A-Za-z0-9_]{3,}', clean_text)[:5]  # å–å‰5ä¸ªæ ¸å¿ƒå®ä½“
                            if entities:
                                entities_str = ", ".join([f"'{e}'" for e in entities])
                                delete_cypher = f"""
                                MATCH (n) 
                                WHERE ANY(prop IN keys(n) WHERE 
                                    toString(n[prop]) IN [{entities_str}]
                                )
                                DETACH DELETE n
                                """
                                self.graph_store.query(delete_cypher)
                                logger.info(f"å·²åˆ é™¤ Neo4j ä¸­ä¸æ–‡æ¡£ {doc_id} å…³è”çš„ä¸‰å…ƒç»„ï¼ˆåŸºäºå®ä½“åŒ¹é…ï¼‰")
                    except:
                        logger.warning(f"æ— æ³•è·å–æ–‡æ¡£ {doc_id} çš„æ–‡æœ¬ï¼Œè·³è¿‡ Neo4j ä¸‰å…ƒç»„åˆ é™¤")
            except Exception as e:
                logger.warning(f"åˆ é™¤ Neo4j ä¸‰å…ƒç»„å¤±è´¥: {str(e)}")
            
            logger.info(f"æ–‡æ¡£ {doc_id} åˆ é™¤å®Œæˆï¼")
            logger.warning(f"æ³¨æ„ï¼šKGç´¢å¼•åˆ é™¤ä¸ºè¿‘ä¼¼åˆ é™¤ï¼Œå¦‚éœ€å®Œå…¨æ¸…ç†ï¼Œå»ºè®®è°ƒç”¨ clear_neo4j_data() åé‡æ–°å¯¼å…¥")
                
        except Exception as e:
            logger.error(f"åˆ é™¤KGæ–‡æ¡£ {doc_id} å¤±è´¥(ç´¢å¼•ID: {KG_RAG_INDEX_ID}): {str(e)}", exc_info=True)
            raise RuntimeError(f"åˆ é™¤KGæ–‡æ¡£å¤±è´¥: {str(e)}")