import os
import re
import tempfile
import logging
import time
from pathlib import Path
from typing import TYPE_CHECKING, AnyStr, BinaryIO, List, Optional, Tuple
from dataclasses import dataclass
from backend_app.constants import get_local_kg_data_path

# é¡¹ç›®å†…éƒ¨ä¾èµ–
from injector import inject, singleton
from backend_app.api.LLM.llm_component import LLMComponent
from backend_app.api.Embedding.embedding_component import EmbeddingComponent
from backend_app.api.LLM.node_store_component import NodeKgStoreComponent
from backend_app.api.llm_api.ingest.model import IngestedDoc
from backend_app.api.settings.settings import settings

# LlamaIndex æ ¸å¿ƒä¾èµ–
from llama_index.core import load_index_from_storage, StorageContext
from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import Document as LlamaDoc
from llama_index.core.storage.docstore.types import RefDocInfo
from llama_index.graph_stores.neo4j import Neo4jGraphStore

from backend_app.api.LLM.vector_store_component import (
    VectorStoreComponent,
)

if TYPE_CHECKING:
    from llama_index.core import QueryEngine
    from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex
import datetime

logger = logging.getLogger(__name__)

# ====================== é…ç½®ç±»ï¼ˆè§£è€¦Neo4jè¿æ¥ï¼‰ ======================
@dataclass
class Neo4jConfig:
    """Neo4j è¿æ¥é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡/é¡¹ç›®é…ç½®è¯»å–ï¼‰"""
    username: str = os.getenv("NEO4J_USER", settings().neo4j.username)
    password: str = os.getenv("NEO4J_PASSWORD", settings().neo4j.password)
    url: str = os.getenv("NEO4J_URL", settings().neo4j.url)
    database: str = os.getenv("NEO4J_DB", settings().neo4j.database)
    max_triplets_per_chunk: int = int(os.getenv("NEO4J_MAX_TRIPLETS", 3))
    include_embeddings: bool = os.getenv("NEO4J_INCLUDE_EMBEDDINGS", "True") == "True"

# ====================== çŸ¥è¯†å›¾è°±RAGæœåŠ¡ï¼ˆå•ä¾‹+ä¾èµ–æ³¨å…¥ï¼‰ ======================
@singleton
class Neo4jKGRAGService:
    """
    çŸ¥è¯†å›¾è°±RAGæœåŠ¡ï¼ˆé€‚é…é¡¹ç›®ç°æœ‰RAGæ¶æ„ï¼‰
    æ ¸å¿ƒè°ƒæ•´ï¼šå¤ç”¨å‘é‡RAGçš„å‘é‡æ•°æ®åº“ï¼Œä¸ºKG-RAGåˆ›å»ºä¸“å±çš„docstore/index_store
    """
    @inject
    def __init__(
        self,
        llm_component: LLMComponent,
        embedding_component: EmbeddingComponent,
        vector_store_component: VectorStoreComponent,
        # ä¿ç•™node_store_componentï¼Œä½†ä»…ä½œä¸ºå‚è€ƒï¼Œä¸å¤ç”¨å…¶å­˜å‚¨
        node_kg_store_component: NodeKgStoreComponent,
        neo4j_config: Neo4jConfig = Neo4jConfig()
    ):
        # å¤ç”¨é¡¹ç›®ç°æœ‰ç»„ä»¶
        self.llm_component = llm_component
        self.embedding_component = embedding_component
        self.node_kg_store_component = node_kg_store_component
        self.vector_store_component = vector_store_component
        self.neo4j_config = neo4j_config
        # 1. åˆå§‹åŒ–Neo4jå›¾è°±å­˜å‚¨ï¼ˆåŸæœ‰é€»è¾‘ï¼Œä¿æŒç‹¬ç«‹ï¼‰
        self.graph_store = self._init_graph_store()

        logger.info(f"âœ… Neo4jå›¾è°±å­˜å‚¨åˆå§‹åŒ–å®Œæˆï¼š{self.neo4j_config}")
        
        # ========== å…³é”®ä¿®å¤ï¼šç¡®ä¿StorageContextå§‹ç»ˆåŒ…å«é»˜è®¤vector_store ==========
        if get_local_kg_data_path().exists():
            # ç›®å½•å­˜åœ¨ä¸”æœ‰æ–‡ä»¶ï¼šä»æœ¬åœ°åŠ è½½StorageContextï¼Œå¹¶å¼ºåˆ¶ç»‘å®švector_store
            logger.info(f"âœ… æ£€æµ‹åˆ°KGæœ¬åœ°å­˜å‚¨ç›®å½•å­˜åœ¨: {get_local_kg_data_path()}ï¼Œå¼€å§‹åŠ è½½æœ¬åœ°ç´¢å¼•")
            self.storage_context = StorageContext.from_defaults(
                persist_dir=get_local_kg_data_path(),  # ä»…æŒ‡å®šæŒä¹…åŒ–ç›®å½•
                graph_store=self.graph_store,
                # å…³é”®ä¿®å¤ï¼šæ˜¾å¼æŒ‡å®šé»˜è®¤vector_store
                vector_store=self.vector_store_component.vector_store
            )
        else:
            # ç›®å½•ä¸å­˜åœ¨/ä¸ºç©ºï¼šé‡æ–°åˆå§‹åŒ–StorageContextï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
            logger.warning(f"âš ï¸ KGæœ¬åœ°å­˜å‚¨ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©º: {get_local_kg_data_path()}ï¼Œé‡æ–°åˆå§‹åŒ–å­˜å‚¨ä¸Šä¸‹æ–‡")
            self.storage_context = StorageContext.from_defaults(
                vector_store=self.vector_store_component.vector_store,
                docstore=self.node_kg_store_component.doc_store,
                index_store=self.node_kg_store_component.index_store,
                graph_store=self.graph_store
            )
        
        # é¢å¤–é˜²æŠ¤ï¼šç¡®ä¿vector_storeså­—å…¸ä¸­æœ‰defaulté”®
        if not hasattr(self.storage_context, 'vector_stores') or 'default' not in self.storage_context.vector_stores:
            logger.warning("âš ï¸ StorageContextç¼ºå°‘default vector_storeï¼Œæ‰‹åŠ¨æ·»åŠ ")
            self.storage_context.vector_stores['default'] = self.vector_store_component.vector_store
            
        logger.info(f"âœ… KGå­˜å‚¨ä¸Šä¸‹æ–‡åˆå§‹åŒ–å®Œæˆ-------------{self.storage_context}")
        # èŠ‚ç‚¹åˆ†å‰²å™¨ï¼ˆä¸åŸæœ‰RAGä½¿ç”¨ç›¸åŒçš„åˆ†å‰²ç­–ç•¥ï¼Œä¿æŒä¸€è‡´ï¼‰
        self.node_parser = SentenceSplitter.from_defaults()
        
        # KGç´¢å¼•å»¶è¿Ÿåˆå§‹åŒ–
        self.kg_index: Optional[KnowledgeGraphIndex] = None

        # åŒé‡æ ¡éªŒç´¢å¼•çŠ¶æ€ï¼ˆNeo4j + æœ¬åœ°æ–‡ä»¶ï¼‰
        self.kg_index_exists = self._check_kg_index_status()
        logger.info(f"âœ… KGç´¢å¼•çŠ¶æ€åŠ è½½å®Œæˆï¼š{'å·²æ„å»º' if self.kg_index_exists else 'æœªæ„å»º'}")
        
        # å¯åŠ¨æ—¶ä¸»åŠ¨åŠ è½½KGç´¢å¼•
        if self.kg_index_exists:
            self._load_kg_index_on_startup()

    def _init_graph_store(self) -> Neo4jGraphStore:
        """åˆå§‹åŒ–Neo4jå›¾è°±å­˜å‚¨ï¼ˆå¼‚å¸¸æ•è·+æ—¥å¿—ï¼ŒåŸæœ‰é€»è¾‘ä¸å˜ï¼‰"""
        try:
            graph_store = Neo4jGraphStore(
                username=self.neo4j_config.username,
                password=self.neo4j_config.password,
                url=self.neo4j_config.url,
                database=self.neo4j_config.database,
            )
            logger.info(f"âœ… æˆåŠŸè¿æ¥Neo4j: {self.neo4j_config.url} (æ•°æ®åº“: {self.neo4j_config.database})")
            return graph_store
        except Exception as e:
            logger.error(f"âŒ Neo4jè¿æ¥å¤±è´¥: {str(e)}", exc_info=True)
            raise ConnectionError(f"Neo4jè¿æ¥å¤±è´¥: {str(e)}")
    
    def _check_kg_index_status(self) -> bool:
        """
        åŒé‡æ ¡éªŒKGç´¢å¼•çŠ¶æ€ï¼š
        1. ä¼˜å…ˆä»Neo4jåŠ è½½çŠ¶æ€
        2. Neo4jçŠ¶æ€ä¸¢å¤±æ—¶ï¼Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨æ–‡ä»¶
        """
        # ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»Neo4jåŠ è½½çŠ¶æ€
        neo4j_status = False
        try:
            neo4j_status = self._load_kg_index_status_from_neo4j()
            if neo4j_status:
                logger.info("âœ… ä»Neo4jæ ¡éªŒåˆ°KGç´¢å¼•å·²æ„å»º")
                return True
        except Exception as e:
            logger.warning(f"âš ï¸ ä»Neo4jæ ¡éªŒç´¢å¼•çŠ¶æ€å¤±è´¥ï¼š{str(e)}")
        
        # ç¬¬äºŒæ­¥ï¼šNeo4jçŠ¶æ€ä¸¢å¤±/æœªæ„å»ºæ—¶ï¼Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨
        local_status = self._check_local_kg_index_files()
        if local_status:
            logger.warning("âš ï¸ Neo4jçŠ¶æ€ä¸¢å¤±ï¼Œä½†æœ¬åœ°å­˜åœ¨ç´¢å¼•æ–‡ä»¶ï¼Œæ ‡è®°ä¸ºå·²æ„å»º")
            # åŒæ­¥çŠ¶æ€åˆ°Neo4j
            self._save_kg_index_status_to_neo4j(True)
            return True
        
        logger.info("â„¹ï¸ æœ¬åœ°å’ŒNeo4jå‡æœªæ£€æµ‹åˆ°KGç´¢å¼•ï¼Œæ ‡è®°ä¸ºæœªæ„å»º")
        return False
    
    def _check_local_kg_index_files(self) -> bool:
        """æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æœ‰æ•ˆçš„KGç´¢å¼•æ–‡ä»¶"""
        kg_path = get_local_kg_data_path()
        if not kg_path.exists():
            return False
        
        # æ£€æŸ¥å…³é”®ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = [
            kg_path / "index_store.json",
            kg_path / "docstore.json",
            kg_path / "vector_store.json"  # æ ¹æ®å®é™…æ–‡ä»¶è°ƒæ•´
        ]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªå…³é”®æ–‡ä»¶å­˜åœ¨ä¸”éç©º
        for file_path in required_files:
            if file_path.exists() and file_path.stat().st_size > 0:
                return True
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦æœ‰å…¶ä»–ç´¢å¼•ç›¸å…³æ–‡ä»¶
        all_files = list(kg_path.glob("*"))
        if len(all_files) > 0:
            return True
        
        return False
    
    def _load_kg_index_status_from_neo4j(self) -> bool:
        """ä»Neo4jåŠ è½½KGç´¢å¼•çŠ¶æ€ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰"""
        if not self.graph_store:
            logger.warning("âš ï¸ Neo4jæœªåˆå§‹åŒ–ï¼Œé»˜è®¤KGç´¢å¼•æœªæ„å»º")
            return False
        
        try:
            load_status_query = "MATCH (n:KGIndexStatus) RETURN n.exists as exists"
            query_results = self.graph_store.query(load_status_query)
            
            if query_results and len(query_results) > 0:
                return query_results[0]["exists"]
            
            logger.warning("âš ï¸ Neo4jä¸­æœªæ‰¾åˆ°KGç´¢å¼•çŠ¶æ€èŠ‚ç‚¹ï¼Œé»˜è®¤ç´¢å¼•æœªæ„å»º")
            return False
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½KGç´¢å¼•çŠ¶æ€å¤±è´¥ï¼Œé»˜è®¤ç´¢å¼•æœªæ„å»ºï¼š{str(e)}")
            return False
    
    def _save_kg_index_status_to_neo4j(self, exists: bool):
        """å°†KGç´¢å¼•çŠ¶æ€æŒä¹…åŒ–åˆ°Neo4jï¼ˆä¿®å¤Cypherè¯­æ³•é”™è¯¯ï¼‰"""
        if not self.graph_store:
            logger.warning("âš ï¸ Neo4jæœªåˆå§‹åŒ–ï¼Œè·³è¿‡KGç´¢å¼•çŠ¶æ€ä¿å­˜")
            return
        
        max_retries = 3  # å¢åŠ é‡è¯•æœºåˆ¶
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # å…ˆåˆ é™¤åŸæœ‰çŠ¶æ€èŠ‚ç‚¹ï¼ˆä¿è¯å”¯ä¸€æ€§ï¼‰
                delete_status_query = "MATCH (n:KGIndexStatus) DELETE n"
                self.graph_store.query(delete_status_query)
                
                # ç®€åŒ–Cypherè¯­å¥ï¼Œç§»é™¤å¤šä½™ç¼©è¿›å’Œæ¢è¡Œ
                create_status_query = """
CREATE (n:KGIndexStatus {exists: $exists, update_time: $update_time, node_desc: "KGç´¢å¼•çŠ¶æ€æ ‡è®°èŠ‚ç‚¹ï¼Œè¯·å‹¿æ‰‹åŠ¨åˆ é™¤", database: $database, version: "1.0"})
                """.strip()  # å»é™¤é¦–å°¾ç©ºç™½
                
                self.graph_store.query(
                    create_status_query,
                    {
                        "exists": exists,
                        "update_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "database": self.neo4j_config.database,
                    }
                )
                
                self.kg_index_exists = exists
                logger.info(f"âœ… KGç´¢å¼•çŠ¶æ€å·²æŒä¹…åŒ–åˆ°Neo4jï¼š{'å·²æ„å»º' if exists else 'æœªæ„å»º'}")
                return
            except Exception as e:
                retry_count += 1
                logger.error(f"âŒ ä¿å­˜KGç´¢å¼•çŠ¶æ€åˆ°Neo4jå¤±è´¥(é‡è¯•{retry_count}/{max_retries}): {str(e)}")
                if retry_count >= max_retries:
                    logger.error(f"âŒ ä¿å­˜KGç´¢å¼•çŠ¶æ€åˆ°Neo4jæœ€ç»ˆå¤±è´¥ï¼Œå°†å°è¯•æœ¬åœ°æ–‡ä»¶æ ‡è®°")
                    # æœ¬åœ°æ–‡ä»¶æ ‡è®°ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
                    self._save_kg_index_status_locally(exists)
                    raise
    
    def _save_kg_index_status_locally(self, exists: bool):
        """æœ¬åœ°æ–‡ä»¶ä¿å­˜ç´¢å¼•çŠ¶æ€ï¼ˆNeo4jå¤±è´¥æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        try:
            status_file = get_local_kg_data_path() / "kg_index_status.json"
            status_file.parent.mkdir(exist_ok=True, parents=True)
            
            import json
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "exists": exists,
                    "update_time": datetime.datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… KGç´¢å¼•çŠ¶æ€å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼š{status_file}")
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°ä¿å­˜KGç´¢å¼•çŠ¶æ€ä¹Ÿå¤±è´¥ï¼š{str(e)}")
    
    def _load_kg_index_on_startup(self):
        """åº”ç”¨å¯åŠ¨æ—¶ä¸»åŠ¨åŠ è½½KGç´¢å¼•ï¼Œé¿å…é¦–æ¬¡æŸ¥è¯¢æ—¶åŠ è½½å¤±è´¥"""
        if not self.kg_index_exists:
            logger.info("â„¹ï¸ KGç´¢å¼•æœªæ„å»ºï¼Œè·³è¿‡å¯åŠ¨æ—¶åŠ è½½")
            return
            
        try:
            logger.info("ğŸ”„ å¯åŠ¨æ—¶ä¸»åŠ¨åŠ è½½KGç´¢å¼•...")
            self.kg_index = load_index_from_storage(
                storage_context=self.storage_context,  # ä½¿ç”¨å·²åˆå§‹åŒ–çš„storage_context
                index_cls=KnowledgeGraphIndex,
            )
            
            # é‡æ–°ç»‘å®šæ ¸å¿ƒç»„ä»¶
            self.kg_index._llm = self.llm_component.llm
            self.kg_index._embed_model = self.embedding_component.embedding_model
            self.kg_index._graph_store = self.graph_store  # æ˜¾å¼ç»‘å®šgraph_store
            self.kg_index._node_parser = self.node_parser
            
            logger.info("âœ… å¯åŠ¨æ—¶KGç´¢å¼•åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨æ—¶åŠ è½½KGç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
            # æ ‡è®°ç´¢å¼•çŠ¶æ€ä¸ºæœªæ„å»ºï¼Œé¿å…åç»­æŸ¥è¯¢æŠ¥é”™
            self.kg_index_exists = False
            self._save_kg_index_status_to_neo4j(False)

    # ====================== æ¸…ç†Neo4jä¸­çš„æ— æ•ˆä¸‰å…ƒç»„ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰ ======================
    def _clean_invalid_triples_in_neo4j(self):
        if not self.graph_store:
            logger.warning("âš ï¸ Neo4jæœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ— æ•ˆä¸‰å…ƒç»„æ¸…ç†")
            return
        
        # å®šä¹‰æ— æ•ˆå…³é”®è¯ï¼ˆä¸æ–‡æœ¬æ¸…ç†é€»è¾‘å¯¹é½ï¼‰
        invalid_keywords = ['E:', 'Tmp', 'tmp', '.txt', 'backend_app', 'llama3.2-projec', 'Backend_app', 'Ai']
        
        try:
            # è½¬ä¹‰å…³é”®è¯ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            escaped_keywords = [keyword.replace("'", "\\'").replace('"', '\\"') for keyword in invalid_keywords]
            keywords_str = ", ".join([f"'{kw}'" for kw in escaped_keywords])
            
            # æŸ¥è¯¢æ‰€æœ‰åŒ…å«æ— æ•ˆå…³é”®è¯çš„èŠ‚ç‚¹
            invalid_node_query = f"""
            MATCH (n) 
            WHERE ANY(keyword IN [{keywords_str}] WHERE 
                ANY(prop IN keys(n) WHERE 
                    toLower(toString(n[prop])) CONTAINS toLower(keyword)
                )
            )
            RETURN elementId(n) as node_id
            """
            invalid_nodes = self.graph_store.query(invalid_node_query)
            
            if not invalid_nodes:
                logger.info("âœ… Neo4jä¸­æ— æ— æ•ˆä¸‰å…ƒç»„ï¼Œæ— éœ€æ¸…ç†")
                return
            
            # åˆ é™¤è¿™äº›æ— æ•ˆèŠ‚ç‚¹åŠå…¶å…³è”çš„å…³ç³»
            delete_invalid_query = f"""
            MATCH (n) 
            WHERE ANY(keyword IN [{keywords_str}] WHERE 
                ANY(prop IN keys(n) WHERE 
                    toLower(toString(n[prop])) CONTAINS toLower(keyword)
                )
            )
            DETACH DELETE n
            """
            self.graph_store.query(delete_invalid_query)
            
            # éªŒè¯æ¸…ç†ç»“æœ
            remaining_triples = self.graph_store.query("MATCH (s)-[r]->(o) RETURN count(*) as total")
            logger.info(f"âœ… æˆåŠŸæ¸…ç†Neo4jä¸­çš„æ— æ•ˆä¸‰å…ƒç»„ï¼š")
            logger.info(f"   - æ¸…ç†çš„æ— æ•ˆèŠ‚ç‚¹æ•°é‡ï¼š{len(invalid_nodes)}")
            logger.info(f"   - æ¸…ç†åå‰©ä½™æœ‰æ•ˆä¸‰å…ƒç»„æ•°é‡ï¼š{remaining_triples[0]['total'] if remaining_triples else 0}")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†Neo4jæ— æ•ˆä¸‰å…ƒç»„å¤±è´¥: {str(e)}", exc_info=True)

    # ====================== æ–‡æ¡£å¤„ç†ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰ ======================
    def _ingest_data(self, file_name: str, file_data: AnyStr) -> list[IngestedDoc]:
        PROJECT_TMP_DIR = Path(__file__).parent.parent.parent.parent / "tmp"
        PROJECT_TMP_DIR.mkdir(exist_ok=True, mode=0o777)
        path_to_tmp = None

        try:
            with tempfile.NamedTemporaryFile(
                dir=str(PROJECT_TMP_DIR),
                suffix=Path(file_name).suffix,
                delete=False
            ) as tmp:
                path_to_tmp = Path(tmp.name)
                if isinstance(file_data, bytes):
                    tmp.write(file_data)
                else:
                    tmp.write(str(file_data).encode("utf-8"))
                tmp.flush()
                os.fsync(tmp.fileno())

            return self.ingest_file(file_name, path_to_tmp)
        finally:
            if path_to_tmp and path_to_tmp.exists():
                try:
                    time.sleep(0.5)
                    path_to_tmp.unlink()
                    logger.debug(f"âœ… ä¸´æ—¶æ–‡ä»¶ {path_to_tmp} å·²æˆåŠŸæ¸…ç†")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼š{str(e)}ï¼Œæ–‡ä»¶å°†æ®‹ç•™ï¼Œå»ºè®®åç»­å®šæ—¶æ¸…ç†")

    def _clean_document_text(self, text: str) -> str:
        if not text:
            return ""
        
        # è¿‡æ»¤è·¯å¾„æ¨¡å¼
        text = re.sub(r'[A-Za-z]:(\\|/)?[^\\/\n]*', '', text)
        text = re.sub(r'^[A-Za-z]:$', '', text, flags=re.MULTILINE)
        
        # è¿‡æ»¤ä¸´æ—¶æ–‡ä»¶å
        text = re.sub(r'Tmp\w+\.txt', '', text)
        
        # è¿‡æ»¤é¡¹ç›®å…³é”®è¯
        project_keywords = r'\b(tmp|Tmp|TEMP|temp|Backend_app|Ai)\b'
        text = re.sub(project_keywords, '', text, flags=re.IGNORECASE)
        
        # è¿‡æ»¤å¤šä½™ç©ºæ ¼å’Œç©ºè¡Œ
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def ingest_file(self, file_name: str, file_data: Path) -> list[IngestedDoc]:
        # 1. åŠ è½½æ–‡æ¡£
        from llama_index.core import SimpleDirectoryReader
        documents = SimpleDirectoryReader(input_files=[file_data]).load_data()
        logger.info(f"åŠ è½½æ–‡ä»¶ {file_name} å®Œæˆï¼ŒåŸå§‹æ–‡æ¡£å—æ•°é‡ï¼š{len(documents)}")

        # 2. æ–‡æ¡£å†…å®¹é¢„å¤„ç†
        processed_docs = []
        for doc in documents:
            clean_text = self._clean_document_text(doc.text)
            if clean_text:
                processed_doc = LlamaDoc(
                    text=clean_text,
                    metadata=doc.metadata,
                    id_=doc.id_
                )
                processed_doc.metadata["original_file_name"] = file_name
                processed_docs.append(processed_doc)
        logger.info(f"æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æ¡£å—æ•°é‡ï¼š{len(processed_docs)}")

        # 3. æ¸…ç©ºå†å²æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if settings().neo4j.clear_existing_data:
            self.clear_neo4j_data()
            logger.info("âœ… å·²æ¸…ç©ºNeo4jç°æœ‰å›¾è°±æ•°æ®")

        # ========== é¢å¤–é˜²æŠ¤ï¼šå†æ¬¡ç¡®è®¤StorageContextçš„vector_store ==========
        if not hasattr(self.storage_context, 'vector_stores') or 'default' not in self.storage_context.vector_stores:
            self.storage_context.vector_stores['default'] = self.vector_store_component.vector_store
        
        # 4. æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•ï¼ˆå¤ç”¨å‘é‡åº“ï¼Œå­˜å‚¨åˆ°KGä¸“å±å­˜å‚¨ï¼‰
        self.kg_index = KnowledgeGraphIndex.from_documents(
            documents=processed_docs,
            storage_context=self.storage_context,
            max_triplets_per_chunk=self.neo4j_config.max_triplets_per_chunk,
            include_embeddings=self.neo4j_config.include_embeddings,
            embed_model=self.embedding_component.embedding_model,
            llm=self.llm_component.llm,
            node_parser=self.node_parser,
            # ä¸‰å…ƒç»„æå–æç¤ºï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
            kg_triple_extract_template="""
            # ä»»åŠ¡è¦æ±‚
            ä»ä»¥ä¸‹æ–‡æœ¬ä¸­ä»…æå–**ä¸šåŠ¡å†…å®¹ç›¸å…³**çš„ä¸‰å…ƒç»„ï¼ˆä¸»ä½“ï¼Œå…³ç³»ï¼Œå®¢ä½“ï¼‰ï¼Œä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

            # è¿‡æ»¤è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰
            1. å®Œå…¨å¿½ç•¥ä»»ä½•ä¸æ–‡ä»¶ç³»ç»Ÿç›¸å…³çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
            - æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ï¼šE:\ã€/home/userã€C:/ï¼‰
            - æ–‡ä»¶åï¼ˆå¦‚ï¼šdocument.txtã€image.pngï¼‰
            - ç›®å½•åï¼ˆå¦‚ï¼štmpã€Backend_appã€Aiï¼‰
            - ç›˜ç¬¦ï¼ˆå¦‚ï¼šC:ã€D:ï¼‰
            2. åªæå–æ–‡æœ¬ä¸­æè¿°å®ä½“ã€å±æ€§ã€å…³ç³»çš„æœ‰æ•ˆä¿¡æ¯ã€‚
            3. ä¸»ä½“å’Œå®¢ä½“å¿…é¡»æ˜¯æœ‰å®é™…ä¸šåŠ¡å«ä¹‰çš„åè¯/çŸ­è¯­ï¼Œå…³ç³»å¿…é¡»æ˜¯èƒ½ä½“ç°ä¸¤è€…å…³è”çš„åŠ¨è¯/ä»‹è¯çŸ­è¯­ã€‚

            # å¥½çš„ç¤ºä¾‹
            - ("Python", "æ˜¯ä¸€ç§", "ç¼–ç¨‹è¯­è¨€")
            - ("ç‰›é¡¿", "æå‡ºäº†", "ä¸‡æœ‰å¼•åŠ›å®šå¾‹")
            - ("ã€Šä¸‰ä½“ã€‹", "çš„ä½œè€…æ˜¯", "åˆ˜æ…ˆæ¬£")

            # åçš„ç¤ºä¾‹ï¼ˆè¯·ä¸è¦è¾“å‡ºè¿™æ ·çš„å†…å®¹ï¼‰
            - ("E:", "IS_LOCATED_IN", "Ai")
            - ("Tmpfile.txt", "HAS_CONTENT", "data")

            # è¾“å‡ºæ ¼å¼ï¼ˆä»…è¿”å›åˆ—è¡¨ï¼Œæ— å…¶ä»–æ–‡å­—ï¼‰
            [("ä¸»ä½“1", "å…³ç³»1", "å®¢ä½“1"), ("ä¸»ä½“2", "å…³ç³»2", "å®¢ä½“2")]

            # éœ€è¦æå–çš„æ–‡æœ¬
            {text}
            """ 
        )
        self.storage_context.persist(persist_dir=get_local_kg_data_path())
        # å¯ç”¨æ— æ•ˆä¸‰å…ƒç»„æ¸…ç†ï¼ˆåŸæœ‰æ³¨é‡Šå–æ¶ˆï¼‰
        #self._clean_invalid_triples_in_neo4j()
        
        # å¼ºåˆ¶æ›´æ–°ç´¢å¼•çŠ¶æ€
        self.kg_index_exists = True
        self._save_kg_index_status_to_neo4j(True)
        
        # 5. ä»Neo4jä¸­è·å–ä¸‰å…ƒç»„å¹¶è¿‡æ»¤æ— æ•ˆæ•°æ®
        try:
            cypher_query = """MATCH (s)-[r]->(o) RETURN s.id AS subject, type(r) AS relation, o.id AS object"""
            query_results = self.graph_store.query(cypher_query)
            all_triples = [
                (result["subject"], result["relation"], result["object"]) 
                for result in query_results
            ]
            
            # åè¿‡æ»¤æ— æ•ˆä¸‰å…ƒç»„
            valid_triples = []
            invalid_keywords = ['E:', 'Tmp', 'tmp', '.txt', 'backend_app', 'llama3.2-projec', 'Backend_app', 'Ai']
            for triple in all_triples:
                if not any(keyword in str(triple) for keyword in invalid_keywords):
                    valid_triples.append(triple)
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±ç´¢å¼•æ„å»ºå®Œæˆï¼š")
            logger.info(f"   - åŸå§‹ä¸‰å…ƒç»„æ•°é‡ï¼š{len(all_triples)}")
            logger.info(f"   - æœ‰æ•ˆä¸‰å…ƒç»„æ•°é‡ï¼š{len(valid_triples)}")
            logger.info(f"   - æœ‰æ•ˆä¸‰å…ƒç»„å†…å®¹ï¼š{valid_triples if valid_triples else 'æ— æœ‰æ•ˆä¸‰å…ƒç»„'}")
            
        except Exception as e:
            logger.error(f"è·å–Neo4jä¸‰å…ƒç»„æ•°é‡å¤±è´¥: {str(e)}", exc_info=True)
            logger.warning(f"âš ï¸ æ— æ³•è·å–ä¸‰å…ƒç»„æ•°é‡ï¼Œå·²é™çº§ä¸º0ï¼ˆæ–‡ä»¶ï¼š{file_name}ï¼‰")

        # 6. æ˜ å°„ä¸ºé¡¹ç›®ç»Ÿä¸€çš„IngestedDocæ¨¡å‹
        current_ingested_docs = [IngestedDoc.from_document(doc) for doc in processed_docs]
        
        # 7. æŸ¥è¯¢KGä¸“å±å­˜å‚¨ä¸­æ‰€æœ‰å·²å…¥åº“çš„å…¨é‡æ–‡æ¡£
        all_ingested_docs = self.list_ingested_kg_docs()
        
        logger.info(f"âœ… å½“å‰ä¸Šä¼ æ–‡æ¡£æ•°ï¼š{len(current_ingested_docs)}ï¼ŒKGä¸“å±å­˜å‚¨å…¨é‡æ–‡æ¡£æ•°ï¼š{len(all_ingested_docs)}")
        return all_ingested_docs

    def ingest_bin_data(self, file_name: str, raw_file_data: BinaryIO) -> list[IngestedDoc]:
        """å¤„ç†äºŒè¿›åˆ¶æ–‡ä»¶æµï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰"""
        try:
            raw_file_data.seek(0)
            file_data = raw_file_data.read()
            return self._ingest_data(file_name, file_data)
        except Exception as e:
            logger.error(f"å¤„ç†äºŒè¿›åˆ¶æ–‡ä»¶ {file_name} å¤±è´¥: {str(e)}", exc_info=True)
            raise

    # ====================== çŸ¥è¯†å›¾è°±RAGæŸ¥è¯¢ï¼ˆä¼˜åŒ–åŠ è½½é€»è¾‘ï¼‰ ======================
    def get_kg_query_engine(self,** kwargs) -> "QueryEngine":
        # å†æ¬¡æ ¡éªŒæœ¬åœ°æ–‡ä»¶
        if not self.kg_index_exists:
            # é‡æ–°æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
            if self._check_local_kg_index_files():
                self.kg_index_exists = True
                self._save_kg_index_status_to_neo4j(True)
            else:
                raise RuntimeError("çŸ¥è¯†å›¾è°±ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")

        # åŒé‡æ£€æŸ¥å¹¶é‡æ–°åŠ è½½ç´¢å¼•
        if not self.kg_index:
            logger.warning("âš ï¸ kg_indexä¸ºç©ºï¼Œå°è¯•é‡æ–°åŠ è½½...")
            self._load_kg_index_on_startup()  # å¤ç”¨å¯åŠ¨åŠ è½½é€»è¾‘
            if not self.kg_index:
                raise RuntimeError("çŸ¥è¯†å›¾è°±ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡æ¡£")

        # é»˜è®¤é…ç½®ï¼ˆå¯é€šè¿‡kwargsè¦†ç›–ï¼‰
        query_config = {
            "include_text": kwargs.get("include_text", True),
            "response_mode": kwargs.get("response_mode", "tree_summarize"),
            "embedding_mode": kwargs.get("embedding_mode", "hybrid"),
            "similarity_top_k": kwargs.get("similarity_top_k", 5),
            "llm": self.llm_component.llm,
            "embed_model": self.embedding_component.embedding_model
        }

        return self.kg_index.as_query_engine(** query_config)

    def query_kg_rag(self, query_text: str, **kwargs) -> str:
        """æ‰§è¡ŒçŸ¥è¯†å›¾è°±RAGæŸ¥è¯¢"""
        try:
            query_engine = self.get_kg_query_engine(** kwargs)
            response = query_engine.query(query_text)
            return str(response)
        except Exception as e:
            logger.error(f"KG RAGæŸ¥è¯¢å¤±è´¥: {str(e)}", exc_info=True)
            raise

    # ====================== è¾…åŠ©æ–¹æ³•ï¼ˆé€‚é…KGä¸“å±å­˜å‚¨ï¼‰ ======================
    def clear_neo4j_data(self) -> None:
        """æ¸…ç©ºNeo4jæ‰€æœ‰èŠ‚ç‚¹/å…³ç³»åŠKGä¸“å±å­˜å‚¨æ•°æ®"""
        if not self.graph_store:
            raise RuntimeError("Neo4jå›¾è°±å­˜å‚¨æœªåˆå§‹åŒ–")
        # æ¸…ç©ºNeo4jå›¾æ•°æ®
        self.graph_store.query("MATCH (n) DETACH DELETE n")
        # æ¸…ç©ºKGä¸“å±æ–‡æ¡£å­˜å‚¨å’Œç´¢å¼•å­˜å‚¨
        self.node_kg_store_component.doc_store.clear()
        self.node_kg_store_component.index_store.clear()
        # é‡ç½®KGç´¢å¼•
        self.kg_index = None
        # åŒæ­¥çŠ¶æ€åˆ°Neo4j
        self.kg_index_exists = False
        self._save_kg_index_status_to_neo4j(False)
        logger.warning("âš ï¸ Neo4jæ‰€æœ‰æ•°æ®åŠKGä¸“å±å­˜å‚¨æ•°æ®å·²æ¸…ç©º")

    def list_ingested_kg_docs(self) -> list[IngestedDoc]:
        """æŸ¥è¯¢KGä¸“å±å­˜å‚¨ä¸­å·²å…¥åº“çš„æ–‡æ¡£ï¼ˆåŸæœ‰é€»è¾‘é€‚é…æ–°å­˜å‚¨ï¼‰"""
        ingested_docs: list[IngestedDoc] = []
        try:
            # ä»KGä¸“å±æ–‡æ¡£å­˜å‚¨ä¸­è·å–å…¨é‡å‚è€ƒæ–‡æ¡£
            ref_docs: dict[str, RefDocInfo] | None = self.node_kg_store_component.doc_store.get_all_ref_doc_info()
            if not ref_docs:
                return ingested_docs

            for doc_id, ref_doc_info in ref_docs.items():
                doc_metadata = None
                if ref_doc_info and ref_doc_info.metadata:
                    doc_metadata = IngestedDoc.curate_metadata(ref_doc_info.metadata)
                ingested_docs.append(
                    IngestedDoc(
                        object="ingest.kg_document",
                        doc_id=doc_id,
                        doc_metadata=doc_metadata, 
                    )
                )
            logger.debug(f"ä»KGä¸“å±å­˜å‚¨æŸ¥è¯¢åˆ° {len(ingested_docs)} ä¸ªå…¥åº“æ–‡æ¡£")
        except Exception as e:
            logger.warning("è·å–KGå…¥åº“æ–‡æ¡£åˆ—è¡¨å¤±è´¥", exc_info=True)
        return ingested_docs

    def delete_kg_doc(self, doc_id: str) -> None:
        """
        åˆ é™¤æŒ‡å®šIDçš„KGæ–‡æ¡£ï¼ˆå…¼å®¹llama-index KGç´¢å¼•ä¸æ”¯æŒåˆ é™¤çš„é™åˆ¶ï¼‰
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. å®‰å…¨åˆ é™¤docstoreå’Œref_doc_infoä¸­çš„æ–‡æ¡£æ•°æ®
        2. æŒä¹…åŒ–ä¿®æ”¹åˆ°æœ¬åœ°å­˜å‚¨
        3. æç¤ºNeo4jä¸‰å…ƒç»„æ— æ³•åˆ é™¤çš„é™åˆ¶
        """
        try:
            logger.info(f"å¼€å§‹åˆ é™¤KGæ–‡æ¡£: {doc_id} (KGç´¢å¼•åˆ é™¤åŠŸèƒ½æš‚ä¸æ”¯æŒï¼Œä»…æ¸…ç†æ–‡æ¡£å­˜å‚¨)")
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿kg_indexå·²åˆå§‹åŒ–
            if self.kg_index is None:
                logger.warning(f"KGç´¢å¼•æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ–‡æ¡£ {doc_id} åˆ é™¤")
                return
            
            # 1. åˆ é™¤docstoreä¸­çš„ä¸»æ–‡æ¡£ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼Œä½ çš„ä»£ç è¿™éƒ¨åˆ†æ˜¯å¯¹çš„ï¼‰
            doc_deleted = False
            if hasattr(self.kg_index, 'docstore') and self.kg_index.docstore:
                if doc_id in self.kg_index.docstore.docs:
                    del self.kg_index.docstore.docs[doc_id]
                    doc_deleted = True
                    logger.info(f"å·²ä»docstoreåˆ é™¤æ–‡æ¡£: {doc_id}")
                else:
                    logger.warning(f"docstoreä¸­æœªæ‰¾åˆ°æ–‡æ¡£: {doc_id}")
            
            # 2. åˆ é™¤ref_doc_infoä¸­çš„å…³è”ä¿¡æ¯ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼Œä½ çš„ä»£ç è¿™éƒ¨åˆ†æ˜¯å¯¹çš„ï¼‰
            ref_deleted = False
            if hasattr(self.kg_index, 'ref_doc_info') and self.kg_index.ref_doc_info:
                if doc_id in self.kg_index.ref_doc_info:
                    del self.kg_index.ref_doc_info[doc_id]
                    ref_deleted = True
                    logger.info(f"å·²ä»ref_doc_infoåˆ é™¤æ–‡æ¡£å…³è”ä¿¡æ¯: {doc_id}")
                else:
                    logger.warning(f"ref_doc_infoä¸­æœªæ‰¾åˆ°æ–‡æ¡£: {doc_id}")
            
            # 3. å…³é”®è¡¥å……ï¼šæŒä¹…åŒ–ä¿®æ”¹åˆ°æœ¬åœ°å­˜å‚¨ï¼ˆä½ çš„ä»£ç ç¼ºå¤±è¿™ä¸€æ­¥ï¼‰
            if doc_deleted or ref_deleted:
                self.storage_context.persist(persist_dir=get_local_kg_data_path())
                logger.info(f"å·²å°†æ–‡æ¡£ {doc_id} çš„åˆ é™¤æ“ä½œæŒä¹…åŒ–åˆ°æœ¬åœ°å­˜å‚¨")
            else:
                logger.warning(f"æ–‡æ¡£ {doc_id} æ— æœ‰æ•ˆæ•°æ®å¯åˆ é™¤")
            
            # 4. é‡è¦æç¤ºï¼šNeo4jä¸­çš„ä¸‰å…ƒç»„æ— æ³•åˆ é™¤ï¼ˆllama-indexé™åˆ¶ï¼‰
            logger.warning(f"æ³¨æ„ï¼šæ–‡æ¡£ {doc_id} å¯¹åº”çš„Neo4jä¸‰å…ƒç»„æœªåˆ é™¤ï¼ˆllama-index KGç´¢å¼•æš‚ä¸æ”¯æŒåˆ é™¤ï¼‰")
            logger.warning(f"å¦‚éœ€å®Œå…¨æ¸…ç†ï¼Œè¯·è°ƒç”¨ clear_neo4j_data() æ¸…ç©ºæ‰€æœ‰KGæ•°æ®åé‡æ–°å¯¼å…¥")
            
        except Exception as e:
            logger.error(f"åˆ é™¤KGæ–‡æ¡£ {doc_id} å¤±è´¥: {str(e)}", exc_info=True)
            raise RuntimeError(f"åˆ é™¤KGæ–‡æ¡£å¤±è´¥: {str(e)}") 